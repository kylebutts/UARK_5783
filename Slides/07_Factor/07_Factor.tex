\documentclass[aspectratio=169,t,11pt,table]{beamer}
\usepackage{../../slides,../../math}
\definecolor{accent}{HTML}{2B5269}
\definecolor{accent2}{HTML}{9D2235}

\title{Topic 6: Factor Models}
\subtitle{\it  ECON 5783 — University of Arkansas}
\date{Fall 2024}
\author{Prof. Kyle Butts}

\usepackage{hhline} % for DID tables

\begin{document}

% ------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
\maketitle

% \bottomleft{\footnotesize $^*$A bit of extra info here. Add an asterich to title or author}
\end{frame}
% ------------------------------------------------------------------------------

\begin{frame}{Imputation Estimator review}
  The last set of slides, we introduced an ``imputation estimator'' for panel data treatment effects:

  \begin{enumerate}
    \item Estimate model for $y_{it}(\infty)$ using observations with $d_{it} = 0$ and get fitted values for full sample, $\hat{y}_{it}(\infty)$
    
    \item Regress $y_{it} - \hat{y}_{it}(\infty)$ on $d_{it}$ or event-study indicators to estimate treatment effects
    \begin{itemize}
      \item Estimating the overall effect, $\text{ATT}$, or dynamic effects of being treated for $\ell$ periods, $\text{ATT}^\ell$ respectively
    \end{itemize}
  \end{enumerate}
  
  \pause
  \bigskip
  This topic will extend this procedure to ``factor models'' that will allow more general trending behavior
\end{frame}

\section{Factor Models}

\begin{frame}{Factor Model}
  Untreated potential outcomes are given by a factor model:
  $$
    y_{it}(0) = \sum_{r = 1}^{p} \purple{f_{t,r}} * \orange{\gamma_{i,r}} + u_{it}
  $$

  \begin{itemize}
    \item $f_{t, r}$ is the $r$-th \textbf{\color{purple} factor} (macroeconomic shock) at time $t$.
    \item $\gamma_{i,r}$ is unit i's \textbf{\color{orange} factor loading} (exposure) to the $r$-th factor.
  \end{itemize}
\end{frame}

\begin{frame}{Factor Model Example}
  \vspace*{-\bigskipamount}
  $$
    y_{it}(0) = \sum_{r = 1}^{p} \purple{f_{t,r}} * \orange{\gamma_{i,r}} + u_{it}
  $$

  \bigskip
  If we are thinking about housing prices, $y_{it}$:
  \begin{itemize}
    \item $\orange{\gamma_i}$ are characteristics of neighborhood / house
    \item $\purple{f_{t}}$ are demand shocks in each period
  \end{itemize}
\end{frame}

\begin{frame}{Factor Model Example}
  \vspace*{-\bigskipamount}
  $$
    y_{it}(0) = \sum_{r = 1}^{p} \purple{f_{t,r}} * \orange{\gamma_{i,r}} + u_{it}
  $$

  \bigskip
  If we are thinking about wages, $y_{it}$:
  \begin{itemize}
    \item $\orange{\gamma_i}$ are worker's latent skills (e.g. computer skills)
    \item $\purple{f_{t}}$ reflect changing firm's demand for skills
  \end{itemize}
\end{frame}

\begin{frame}{Factor Model Example}
  \vspace*{-\bigskipamount}
  $$
    y_{it}(0) = \sum_{r = 1}^{p} \purple{f_{t,r}} * \orange{\gamma_{i,r}} + u_{it}
  $$

  \bigskip
  If we are thinking about county employment, $y_{it}$:
  \begin{itemize}
    \item $\orange{\gamma_i}$ are characteristics of a county (e.g. their manufacturing share)
    \item $\purple{f_{t}}$ reflect national shocks to the economy (e.g. the ``China shock'')
  \end{itemize}
\end{frame}

\begin{frame}{Two-way Fixed Effect vs. Factor Model}
  The factor model is a generalization of the TWFE model. If $\bm{f}_{t} = (\lambda_t, 1)'$ and $\bm{\gamma}_i = (1, \mu_i)'$, then our factor model becomes the TWFE model:
  $$
    y_{it}(0) = \bm{f}_t' \bm{\gamma}_i + u_{it} = \lambda_t + \mu_i + u_{it}
  $$
  
  \bigskip
  We can add unit and/or time fixed-effects as `known' factors if we want
\end{frame}


\begin{frame}{Factor Model and Parallel Trends}
  Say you have a single treatment timing and two periods. Let $D_i$ be out treated group indicator. Then
  \begin{align*}
    \expec{\Delta y_{i}}{D_i = d} 
    &= \expec{y_{i1} - y_{i0}}{D_i = d} \\ 
    &= \Delta \bm{f} \expec{\bm{\gamma}_i}{D_i = d} 
  \end{align*}
  
  \bigskip
  Under a factor model, the average change in $y_{it}$ for group $D_i = d$ is the change in factor shocks $\bm{f}$ times the average exposure to those shocks 
\end{frame}

\begin{frame}{Factor Model and Parallel Trends}
  \vspace*{-\bigskipamount}
  $$ 
    \expec{\Delta y_{i}}{D_i = d} = \Delta \bm{f} \expec{\bm{\gamma}_i}{D_i = d} 
  $$
  
  Say the treated group has higher exposure to a shock than the control group
  \begin{itemize}
    \item $\implies$ the trends differ by treatment status
  \end{itemize}

  \pause
  \bigskip
  That is, a factor model allows for ``non-parallel trends'' based on difference in exposures to shocks
\end{frame}

\begin{frame}{Example}
  \vspace*{-\bigskipamount}
  $$
    y_{it}(0) = \sum_{r = 1}^{p} \purple{f_{t,r}} * \orange{\gamma_{i,r}} + u_{it}
  $$

  \bigskip
  Say we are thinking about neighborhood housing prices, $y_{nt}$. We are interested in some treatment $D_n$, e.g. access to subways. 
  \begin{itemize}
    \item Say $\orange{\gamma_n}$ is the walk-ability of the neighborhood
    \item $\purple{f_{t}}$ are demand shocks for walkable neighborhoods
  \end{itemize}

  \bigskip
  If new subways are built in more walkable neighborhoods, then we do not believe parallel trends hold in this setting
\end{frame}


\begin{frame}{Factor Loadings are `non-observed $X_i$'}
  In the worker's wage example, we might suspect that something like computer-skill might be unobservable and have a time-varying impact. 
  
  \bigskip That is both $X_i$ and $\beta_t$ are unobserved!

  \begin{itemize}
    \item If the job-training program attracts people with more computer-skills (e.g. excel workshop), then PTs does not hold.
    \item We can not `compare two individuals with the same value of $X_i$' since we do not observe them.
  \end{itemize}
\end{frame}

\begin{frame}{Factor Models}
  $$
    Y_{it} = \sum_{r=1}^\rho \lambda_{i, r} f_{t, r} + \varepsilon_{it}
  $$

  \bigskip
  Let's give an example using county-level aggregate employment. 
  \begin{itemize}
    \item $\lambda_i$ might consist of (i) manufacturing share and (ii) share of college-educated
    \item In each period, shocks to the national economy change manufacturing demand and (ii) technological change drives returns to college degree 
  \end{itemize}

  \pause
  \bigskip 
  We might have ideas of what are the primary characteristics are, but we might not have data on it. If you do, then we are back in $X_i$ land.
\end{frame}

\begin{frame}{Imputation and (Linear) Factor Models}
  We have a more-general model for $Y_{it}(0)$ now that allows some forms of non-parallel-trends:
  $$
    Y_{it}(0) =  \mu_i + \eta_t + \sum_{r=1}^\rho \lambda_{i, r} f_{t, r} + \varepsilon_{it}
  $$

  \bigskip\bigskip
  Can we estimate this and use our imputation procedure:
  \begin{itemize}
    \item The short-answer is yes. There is a bunch of different approaches but they're not as simple as TWFE. 

    \item The factor model is much more data-hungry than fixed effects, usually requiring both a large number of units \emph{ and } a large number of time-periods.
  \end{itemize}
\end{frame}


\begin{frame}{`Extensions' of the Synthetic Control Model}{Generalized Imputation Estimator}
  Xu (2017) and Gobillon and Magnac (2016) both discuss using an imputation estimator under a \emph{factor model} 

  \bigskip
  Intuition is to estimate $\lambda_{i,r}$ and $f_{t,r}$ using untreated/not-yet-treated observations and then impute:
  $$
    \hat{Y}_{it}(0) = \hat{\mu}_i + \hat{\lambda}_t + \sum_{r=1}^\rho \hat{\lambda}_{i, r} \hat{f}_{t, r}
  $$
  \begin{itemize}
    \item We estimate the non-parallel trending via the factor model and then subtract it off
    \begin{itemize}
      \item Can implement this using the package \texttt{gsynth}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Factor Model Imputation}
  \vspace*{-\bigskipamount}
  $$
    Y_{it}(0) =  \mu_i + \eta_t + \sum_{r=1}^\rho \lambda_{i, r} f_{t, r} + \varepsilon_{it}
  $$

  \bigskip
  Unlike imputation of the two-way fixed effect model, this approach is very data hungry:
  \begin{itemize}
    \item Requires both a large number of time-periods and units
  \end{itemize}

  \bigskip
  Intuitively, you need a long number of time-periods to estimate a unit's $\lambda_{i,r}$. You need a large number of units to estimate a time-period's $f_{t,r}$
\end{frame}

\begin{frame}{Problem with large-$T$}
  Estimation with a factor model is more robust than standard DID, but rely on having access to many years of data.
  In a lot of applied work, the data is just not available

  \pause
  \bigskip
  But I think there is a more subtle problem at play with this assumption: 
  \begin{itemize}
    \item Data from many years ago might not be very useful at understanding the underlying confounders at play in this economy
    \item Imagine saying "I use data from 1960 to inform me which counties would be a good control group for housing prices in 2000". A lot has happened since then !!!!!!
  \end{itemize}
\end{frame}


\section{Synthetic Control}

\begin{frame}{Synthetic Control}
  The standard synthetic control method considers a single treated unit (country, state, firm, etc.). We will let $\bm{Y}_i = \left( Y_{i1}, \dots, Y_{iT} \right)'$ be the vector of outcomes for unit $i$. 
  \begin{itemize}
    \item The treated unit is $i = 0$. We have $N$ control units
  \end{itemize}

  \bigskip\pause
  Synthetic control imputes $\bm{Y}_0(0)$ for the treated unit using a weighted average of the control units: 
  $$
    \hat{\bm{Y}}_0(0) = \sum_{i = 1}^N w_i \bm{Y}_i
  $$
  \begin{itemize}
    \item Take a part of this state, a part of that state, and then average them together into a `synthetic control' unit
    \item In most cases, we use convex weights $1 \geq w_i \geq 0$.
  \end{itemize}
\end{frame}


\begin{frame}{Choosing weights}
  We want our synthetic control unit to do a good job at approximating the pathway of outcomes for the treated unit: $\bm{Y}_0(0)$.

  \pause
  \bigskip
  We only observe $Y_{0t}(0)$ for the treated unit up until period $T_0$ which is the period prior to treatment. 
  \begin{itemize}
    \item Synthetic control should try to match the treated unit's outcome path during the pre-period and \emph{HOPEFULLY} that will mean the synthetic control will do a good job in the post-period. It's a leap of faith
  \end{itemize}
\end{frame}

\begin{frame}{Identifying assumption}
  The identifying assumption can be said as:
  ``The synthetic control unit approximates the counterfactual trend that the treated unit would be on had they not been treated.''

  \begin{itemize}
    \item This is like a parallel trends assumption between the treated unit and the synthetic control unit
  \end{itemize}
\end{frame}

\begin{frame}{Choosing weights}
  More formally, the weights are selected by trying to minimize the following:
  $$
    \argmin_{ \{ w_i \} } \sum_{t=1}^{T_0} \left( Y_{0t} - \sum_{i=1}^N w_i Y_{it} \right)^2
  $$
  \begin{itemize}
    \item Minimizing the \emph{pre-treatment} sum of squared prediction errors between treated unit and the synthetic control unit.
    \item With convex weights, $1 \geq w_i \geq 0$, we add these as a constrained optimization problem
    \begin{itemize}
      \item Convex weights avoid having `-0.2 Minnesota'
    \end{itemize}
    % \item Sometimes pre-treatment covariates are used and squared error is summed across the covariates
  \end{itemize}
\end{frame}


\begin{frame}{Example: California's Proposition 99}
	In 1988, California first passed comprehensive tobacco control legislation:
  \begin{itemize}
		\item increased cigarette tax by 25 cents/pack
		\item earmarked tax revenues to health and anti-smoking budgets
		\item funded anti-smoking media campaigns
		\item spurred clean-air ordinances throughout the state
		\item produced more than \$100 million per year in anti-tobacco projects
  \end{itemize}
	
  \bigskip Other states that subsequently passed control programs are excluded from donor pool of controls (AK, AZ, FL, HI, MA, MD, MI, NJ, OR, WA, DC)
\end{frame}

% \begin{frame}{Cigarette Consumption: CA and the Rest of the US}
\imageframe{figures/abadie_3.pdf}
% \end{frame}

% \begin{frame}{Cigarette Consumption: CA and synthetic CA}
\imageframe{figures/abadie_4.pdf}
% \end{frame}

% \begin{frame}{Sparsity and Synthetic Control Weights}
\imageframe{figures/synth_smoking_table2.png}
% \end{frame}

\begin{frame}{Identifying assumption}
  The identifying assumption can be said as:
  ``The synthetic control unit approximates the counterfactual trend that the treated unit would be on had they not been treated.''

  \bigskip
  We select our weights to make the pre-treatment trends very similar between the treated and synthetic control 
  \begin{itemize}
    \item Unlike the `pre-trends' test in DID, we are sort of `cheating' here in that we are making the synthetic control do that.
  \end{itemize}

  \pause
  \bigskip
  But, the idea is that as the number of pre-periods grow, you can only do a good job at matching the treated $y$ in \emph{all} pre-periods if your synthetic control ``\emph{looks similar} to treated unit''
\end{frame}

\begin{frame}{\emph{Beware of Overfitting}}
  Intuitively, synthetic control is `believable' when the synthetic control unit does a great job at approximating the outcome in the per-period
  \begin{itemize}
    \item We think that the synthetic control must be picking up on underlying economic structure in order to co-move with the treated state.
  \end{itemize}

  \bigskip
  E.g. if you see a set of runners whose times go up and go down during the same races, you might think they train together. 
  \begin{itemize}
    \item You wouldn't think that chance alone made them have the same ups and downs.
  \end{itemize}
\end{frame}

\begin{frame}{\emph{Beware of Overfitting}}
  The number one concern you should have when reading or writing a paper that uses a synthetic control method is that of overfitting. 
  \begin{itemize}
    \item If I have 1000 control units and 4 pre-periods, I can probably well approximate $Y_{0t}$ in the pre-periods by just random chance. 
  \end{itemize}
\end{frame}

\begin{frame}{Synthetic Control and Factor Model}
  Recall our factor model:
  \begin{itemize}
    \item There are a set of characteristics that the units have and in each period a set of macroeconomic shocks that change the marginal effect of those characteristics
    \item If we could observe these characteristics, we would want to match on them or use them in conditional PTs
  \end{itemize}

  \pause
  \bigskip
  Synthetic control, under conditions we will discuss, will create a synthetic control unit that has the same average characteristics as the treated unit! 
  \begin{itemize}
    \item Since they are subject to the shocks the same amount, we think their outcome evolutions will match.
  \end{itemize}
\end{frame}

\begin{frame}{Synthetic Control and Factor Model}
  Okay, so I've pained a very rosy picture of synthetic control. When it works well, we fix the problem of non-parallel trends and we are able to impute the untreated potential outcome well. 
  
  \bigskip
  However, I want to make clear that this method is \textbf{not a panacea}. The original paper is very general and makes it hard to know when it works and when it does not. 
  \begin{itemize}
    \item More recent advancements all base discussion on a factor model for outcomes
    \item Hollingsworth and Wing working paper has great discussion
  \end{itemize} 
\end{frame}

\begin{frame}{Bias of Synthetic Control}
  The original Abadie, Diamond and Hainmueller paper derive the bias of synthetic control when outcomes are generated by a linear factor-model. 
  
  \bigskip
  The bias arises from \textbf{over-fitting on noise}. It is more common to over-fit on data when: 
  \begin{itemize}
    \item There are fewer pre-treatment periods $T_0$
    
    \item There are many control units
    
    \item The `convex hull' assumption is unlikely to ˙old
  \end{itemize}
\end{frame}

\begin{frame}{`Convex Hull' Assumption}
  When constraining the weights to be convex ($0 \leq w_i \leq 1$), the synthetic control assumption requires the `convex hull' assumption:
  \begin{itemize}
    \item This is basically an assumption that says `we can approximate $\bm{Y}_{0}$ using a convex weighted average of control $\bm{Y}_i$'
  \end{itemize}

  \pause
  \bigskip
  With a factor model, we can make this assumption a lot clearer:
  \begin{itemize}
    \item The `convex hull' assumption is equivalent to the assumption that the treated unit's `factor loadings' are a convex average of the other units' `factor loadings'
    \begin{itemize}
      \item That is, the treated unit can not have an extreme value in any of the $\lambda_{i,r}$ (e.g. huge manufacturing share)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Inference in Synthetic Control}
  Inference in the classical synthetic control setting is really difficult
  \begin{itemize}
    \item Only one treated unit; you're not averaging over units so the estimate is subject to random shocks
  \end{itemize}

  \pause
  \bigskip
  Two forms of randomization inference are typically used:
  \begin{itemize}
    \item Randomly shuffle treatment to control units and re-estimate synthetic control; want the treated unit to look more extreme than the placebo estimates. The so-called `spaghetti plot'
    
    \item Randomly shuffle time-periods around for treated unit and re-estimate synthetic control
  \end{itemize}
\end{frame}

\begin{frame}{Implementing Synthetic Control}
  My recommendation for synthetic control is \texttt{scpi} package (on R, Stata, and Python)
  \begin{itemize}
    \item Covers all the basic method and include inference methods

    \item Journal of Statistical Software paper is super readable: \url{https://nppackages.github.io/references/Cattaneo-Feng-Palomba-Titiunik_2024_JSS.pdf}
  \end{itemize}

\end{frame}


\section{More General Factor Model Imputation}

\begin{frame}{`Extensions' of the Synthetic Control Model}{Synthetic Control with Lasso/Ridge Penalty}
  People seem to like when the synthetic control is made up of a few units
  \begin{itemize}
    \item Makes the control unit more `interpretable'
  \end{itemize}
\end{frame}

\begin{frame}{`Extensions' of the Synthetic Control Model}{Synthetic Control with Lasso/Ridge Penalty}
  Can modify the weights optimization problem to penalize weights being too large:
  $$
    \argmin_{ \{ w_i \} } \sum_{t=1}^{T_0} \left( Y_{0t} - \sum_{i=1}^N w_i Y_{it} \right)^2 + \lambda \| w \|_k
  $$
  \begin{itemize}
    \item Add a term that punishes when weights are non-zero; $\lambda$ is a `tuning-parameter' to chose how much to punish
    \item Can add convex-weights constraint
    \item Lasso is $k = 1$; Ridge is $k = 2$
  \end{itemize}
\end{frame}


\begin{frame}{`Extensions' of the Synthetic Control Model}{Augmented Synthetic Control}
  Augmented control is a method to help with imperfect pre-treatment fit by estimating a bias and subtracting it off. 
  \begin{itemize}
    \item Looks similar to regression adjustment. We estimate the trend using covariates and then use that model to bias correct. 
  \end{itemize}
\end{frame}

\begin{frame}{`Extensions' of the Synthetic Control Model}{Augmented Synthetic Control}
  \begin{enumerate}
    \item Calculate the standard synthetic control method weights (or with lasso)
    \item Estimate a model of $m_{t}(Z_i) = \expec{Y_{it}}{Z_i}$ using untreated units where $Z_i$ is pre-treatment characteristics (lagged $Y$ or covariates)
  \end{enumerate}

  \bigskip
  Form synthetic control estimate as 
  $$
    \left( Y_{0t} - \sum_{i = 1}^N w_i Y_{it} \right) -
      \underbrace{\left( m_{t}(Z_0) - \sum_{i = 1}^N w_i m_t(Z_i) \right)}_{\text{`bias correction'}}
  $$
\end{frame}







\end{document}
